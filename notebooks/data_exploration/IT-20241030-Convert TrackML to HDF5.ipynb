{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a95fd541-f6bd-4bd0-b590-59bb27e18b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import h5py\n",
    "from pyarrow import csv, ArrowInvalid\n",
    "from tqdm import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c626767-c352-41eb-a4d5-fbbe7f257a47",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3054a280-7625-4a8b-a297-a8aca6d27ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the paths below as needed\n",
    "DATA_ROOT = Path('/home/ucloud/data')\n",
    "OUTPUT_HDF5_FILE = Path('../data/TrackML/training-small')\n",
    "\n",
    "HDF5_GROUP_KWARGS = {'track_order': True}\n",
    "HDF5_DATASET_KWARGS = {'fletcher32': True, 'chunks': True, 'compression': 'lzf'}\n",
    "#HDF5_DATASET_KWARGS = {'fletcher32': True, 'chunks': True, 'compression': 'gzip', 'compression_opts': 9}  # Use GZIP if the filesystem is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159d78f-4085-4df0-9546-fb60d3ec34e1",
   "metadata": {},
   "source": [
    "## Handle corrupted or missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155580cf-5e5c-41a4-ba9e-1d3dafb4b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELETE_INVALID_FILES = True\n",
    "INVALID_FILES = [\n",
    "    'event000022500-cells.csv.gz',\n",
    "    'event000023450-cells.csv.gz',\n",
    "    'event000023161-truth.csv.gz',\n",
    "    'event000023157-cells.csv.gz',\n",
    "]\n",
    "\n",
    "for file in INVALID_FILES:\n",
    "    event_number = file[5:14]\n",
    "    invalid_group = DATA_ROOT.glob(f'event{event_number}*.csv.gz')\n",
    "    for related_file in invalid_group:\n",
    "        if DELETE_INVALID_FILES and related_file.exists():\n",
    "            related_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe3a9c-314c-4146-8757-145438f733f9",
   "metadata": {},
   "source": [
    "## Check the number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0373bc66-f4b5-4ba7-bb29-068ea4361156",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_files = list(DATA_ROOT.glob('event?????????-hits.csv.gz'))\n",
    "cells_files = list(DATA_ROOT.glob('event?????????-cells.csv.gz'))\n",
    "particles_files = list(DATA_ROOT.glob('event?????????-particles.csv.gz'))\n",
    "truth_files = list(DATA_ROOT.glob('event?????????-truth.csv.gz'))\n",
    "\n",
    "assert len(hits_files) == len(cells_files) == len(particles_files) == len(truth_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c9fd4-909f-46c8-99f4-d83392af9f89",
   "metadata": {},
   "source": [
    "## Write hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3c4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_HDF5_FILE.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08bd0c7-575e-493d-9659-477beba59aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:58<00:00, 15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 10.9 s, total: 1min 45s\n",
      "Wall time: 58.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with h5py.File(OUTPUT_HDF5_FILE, 'w', **HDF5_GROUP_KWARGS) as fp:\n",
    "\n",
    "    fp.attrs['number_of_events'] = len(hits_files)\n",
    "    \n",
    "    hits = fp.create_group('hits', **HDF5_GROUP_KWARGS)\n",
    "\n",
    "    # Event-level data\n",
    "    event_offset = hits.create_dataset('event_offset', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "    event_length = hits.create_dataset('event_length', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "\n",
    "    # Individual hits\n",
    "    hit_id = hits.create_dataset('hit_id', (0,), maxshape=(None,), dtype='i4', **HDF5_DATASET_KWARGS)\n",
    "    x = hits.create_dataset('x', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    y = hits.create_dataset('y', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    z = hits.create_dataset('z', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    volume_id = hits.create_dataset('volume_id', (0,), maxshape=(None,), dtype='i1', **HDF5_DATASET_KWARGS)\n",
    "    layer_id  = hits.create_dataset( 'layer_id', (0,), maxshape=(None,), dtype='i1', **HDF5_DATASET_KWARGS)\n",
    "    module_id = hits.create_dataset('module_id', (0,), maxshape=(None,), dtype='i2', **HDF5_DATASET_KWARGS)\n",
    "\n",
    "    current_event_offset = 0\n",
    "    for file in tqdm(hits_files):\n",
    "        try:\n",
    "            table = csv.read_csv(file)\n",
    "        except ArrowInvalid as ex:\n",
    "            logging.warning(f'Invalid file {file}')\n",
    "            continue\n",
    "        N_hits = table.shape[0]\n",
    "        \n",
    "        for group in [event_offset, event_length]:\n",
    "            group.resize(group.shape[0] + 1, axis=0)\n",
    "        event_length[-1] = N_hits\n",
    "        event_offset[-1] = current_event_offset\n",
    "        current_event_offset += N_hits\n",
    "\n",
    "        for group in [hit_id, x, y, z, volume_id, layer_id, module_id]:\n",
    "            group.resize(group.shape[0] + N_hits, axis=0)\n",
    "        hit_id[-N_hits:] = table['hit_id']\n",
    "        x[-N_hits:] = table['x']\n",
    "        y[-N_hits:] = table['y']\n",
    "        z[-N_hits:] = table['z']\n",
    "        volume_id[-N_hits:] = table['volume_id']\n",
    "        layer_id[ -N_hits:] = table[ 'layer_id']\n",
    "        module_id[-N_hits:] = table['module_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59f3b8-1aed-4592-8ca5-b79607f4059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0352ee8-36ec-4629-8f66-9a9973e72540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [01:41<00:00,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 13.4 s, total: 3min 2s\n",
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with h5py.File(OUTPUT_HDF5_FILE, 'r+') as fp:\n",
    "    \n",
    "    cells = fp.create_group('cells', **HDF5_GROUP_KWARGS)\n",
    "\n",
    "    # Event-level data\n",
    "    event_offset = cells.create_dataset('event_offset', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "    event_length = cells.create_dataset('event_length', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "\n",
    "    # Individual hits\n",
    "    hit_id = cells.create_dataset('hit_id', (0,), maxshape=(None,), dtype='i4', **HDF5_DATASET_KWARGS)\n",
    "    ch0 = cells.create_dataset('ch0', (0,), maxshape=(None,), dtype='i2', **HDF5_DATASET_KWARGS)\n",
    "    ch1 = cells.create_dataset('ch1', (0,), maxshape=(None,), dtype='i2', **HDF5_DATASET_KWARGS)\n",
    "    value = cells.create_dataset('value', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "\n",
    "    current_event_offset = 0\n",
    "    for file in tqdm(cells_files):\n",
    "        try:\n",
    "            table = csv.read_csv(file)\n",
    "        except ArrowInvalid as ex:\n",
    "            logging.warning(f'Invalid file {file}')\n",
    "            continue\n",
    "        N_hits = table.shape[0]\n",
    "        \n",
    "        for group in [event_offset, event_length]:\n",
    "            group.resize(group.shape[0] + 1, axis=0)\n",
    "        event_length[-1] = N_hits\n",
    "        event_offset[-1] = current_event_offset\n",
    "        current_event_offset += N_hits\n",
    "\n",
    "        for group in [hit_id, ch0, ch1, value]:\n",
    "            group.resize(group.shape[0] + N_hits, axis=0)\n",
    "        hit_id[-N_hits:] = table['hit_id']\n",
    "        ch0[-N_hits:] = table['ch0']\n",
    "        ch1[-N_hits:] = table['ch1']\n",
    "        value[-N_hits:] = table['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcd7d9-ac64-461b-9091-578348c7916b",
   "metadata": {},
   "source": [
    "## Write particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1bb5317-9234-4c4e-9f6c-0f336c631faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:26<00:00, 33.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.8 s, sys: 4.95 s, total: 37.8 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with h5py.File(OUTPUT_HDF5_FILE, 'r+') as fp:\n",
    "    \n",
    "    particles = fp.create_group('particles', **HDF5_GROUP_KWARGS)\n",
    "\n",
    "    # Event-level data\n",
    "    event_offset = particles.create_dataset('event_offset', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "    event_length = particles.create_dataset('event_length', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "\n",
    "    # Individual particles\n",
    "    particle_id = particles.create_dataset('particle_id', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "    particle_type = particles.create_dataset('particle_type', (0,), maxshape=(None,), dtype='i2', **HDF5_DATASET_KWARGS)\n",
    "    vx = particles.create_dataset('vx', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    vy = particles.create_dataset('vy', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    vz = particles.create_dataset('vz', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    px = particles.create_dataset('px', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    py = particles.create_dataset('py', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    pz = particles.create_dataset('pz', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    q = particles.create_dataset('q', (0,), maxshape=(None,), dtype='i1', **HDF5_DATASET_KWARGS)\n",
    "    nhits = particles.create_dataset('nhits', (0,), maxshape=(None,), dtype='i1', **HDF5_DATASET_KWARGS)\n",
    "    \n",
    "    current_event_offset = 0\n",
    "    for file in tqdm(particles_files):\n",
    "        try:\n",
    "            table = csv.read_csv(file)\n",
    "        except ArrowInvalid as ex:\n",
    "            logging.warning(f'Invalid file {file}')\n",
    "            continue\n",
    "        N_particles = table.shape[0]\n",
    "        \n",
    "        for group in [event_offset, event_length]:\n",
    "            group.resize(group.shape[0] + 1, axis=0)\n",
    "        event_length[-1] = N_particles\n",
    "        event_offset[-1] = current_event_offset\n",
    "        current_event_offset += N_particles\n",
    "\n",
    "        for group in [particle_id, particle_type, vx, vy, vz, px, py, pz, q, nhits]:\n",
    "            group.resize(group.shape[0] + N_particles, axis=0)\n",
    "        particle_id[-N_particles:] = table['particle_id']\n",
    "        particle_type[-N_particles:] = table['particle_type']\n",
    "        vx[-N_particles:] = table['vx']\n",
    "        vy[-N_particles:] = table['vy']\n",
    "        vz[-N_particles:] = table['vz']\n",
    "        px[-N_particles:] = table['px']\n",
    "        py[-N_particles:] = table['py']\n",
    "        pz[-N_particles:] = table['pz']\n",
    "        q[-N_particles:] = table['q']\n",
    "        nhits[ -N_particles:] = table[ 'nhits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe6260-2a5d-43e2-a16c-cadb8dfc143e",
   "metadata": {},
   "source": [
    "## Write truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11cc7c8f-4095-4aed-9ecb-4571f44091d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [01:52<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 7s, sys: 17.5 s, total: 3min 25s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with h5py.File(OUTPUT_HDF5_FILE, 'r+') as fp:\n",
    "    \n",
    "    truth = fp.create_group('truth', **HDF5_GROUP_KWARGS)\n",
    "\n",
    "    # Event-level data\n",
    "    event_offset = truth.create_dataset('event_offset', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "    event_length = truth.create_dataset('event_length', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "\n",
    "    # Individual hits\n",
    "    hit_id = truth.create_dataset('hit_id', (0,), maxshape=(None,), dtype='i4', **HDF5_DATASET_KWARGS)\n",
    "    particle_id = truth.create_dataset('particle_id', (0,), maxshape=(None,), dtype='i8', **HDF5_DATASET_KWARGS)\n",
    "    tx = truth.create_dataset('tx', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    ty = truth.create_dataset('ty', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    tz = truth.create_dataset('tz', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    tpx = truth.create_dataset('tpx', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    tpy = truth.create_dataset('tpy', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    tpz = truth.create_dataset('tpz', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    weight = truth.create_dataset('weight', (0,), maxshape=(None,), dtype='f4', **HDF5_DATASET_KWARGS)\n",
    "    \n",
    "    current_event_offset = 0\n",
    "    for file in tqdm(truth_files):\n",
    "        try:\n",
    "            table = csv.read_csv(file)\n",
    "        except ArrowInvalid as ex:\n",
    "            logging.warning(f'Invalid file {file}')\n",
    "            continue\n",
    "        N_hits = table.shape[0]\n",
    "        \n",
    "        for group in [event_offset, event_length]:\n",
    "            group.resize(group.shape[0] + 1, axis=0)\n",
    "        event_length[-1] = N_hits\n",
    "        event_offset[-1] = current_event_offset\n",
    "        current_event_offset += N_hits\n",
    "\n",
    "        for group in [hit_id, particle_id, tx, ty, tz, tpx, tpy, tpz, weight]:\n",
    "            group.resize(group.shape[0] + N_hits, axis=0)\n",
    "        hit_id[-N_hits:] = table['hit_id']\n",
    "        particle_id[-N_hits:] = table['particle_id']\n",
    "        tx[-N_hits:] = table['tx']\n",
    "        ty[-N_hits:] = table['ty']\n",
    "        tz[-N_hits:] = table['tz']\n",
    "        tpx[-N_hits:] = table['tpx']\n",
    "        tpy[-N_hits:] = table['tpy']\n",
    "        tpz[-N_hits:] = table['tpz']\n",
    "        weight[-N_hits:] = table['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d93d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
