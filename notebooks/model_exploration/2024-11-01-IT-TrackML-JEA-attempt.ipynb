{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, Callable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jepa.modules import JEA\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackMLDataset(Dataset):\n",
    "    r\"\"\"\n",
    "    A Dataset subclass for the TrackML dataset in HDF5 format.\n",
    "\n",
    "    Args:\n",
    "        file (str or Path): path to the HDF5 file holding the data.\n",
    "        scaling_factor (float, optional): a multiplicative scaling factor applied to the hit positions (default: ``1.0``).\n",
    "            Note that, by default, positions are specified in millimeters.\n",
    "        transform (Callable, optional): a function used to further process the output.\n",
    "        float_dtype (torch.dtype, optional): the dtype of the returned tensors for floating-point features (default: ``torch.float32``).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file: Union[str,Path],\n",
    "        scaling_factor: float=1.0,\n",
    "        transform: Optional[Callable]=None, #TODO transforms\n",
    "        float_dtype=torch.float32,\n",
    "    ):\n",
    "        super(TrackMLDataset).__init__()\n",
    "        self.file = h5py.File(file, 'r')\n",
    "        self.number_of_events = self.file.attrs['number_of_events']\n",
    "        self.hits = self.file['hits']\n",
    "        self.truth = self.file['truth']\n",
    "        self.float_dtype = float_dtype\n",
    "        self.scaling_factor = torch.tensor(scaling_factor, dtype=float_dtype)\n",
    "        self.transform = transform #TODO transforms\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_events\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x, hit_id = self._get_hits(idx)\n",
    "        pids = self._get_particle_ids(idx, hit_id)\n",
    "        output = {\n",
    "            'x': x,\n",
    "            'mask': torch.ones(x.shape[0], dtype=bool),\n",
    "            'pids': pids,\n",
    "            'event': None,\n",
    "        }\n",
    "        if self.transform:\n",
    "            output = self.transform(output)\n",
    "        return output\n",
    "\n",
    "    def _get_hits(self, idx: int):\n",
    "        offset = self.hits['event_offset'][idx]\n",
    "        length = self.hits['event_length'][idx]\n",
    "        event_slice = slice(offset, offset+length)\n",
    "        hit_id = pd.DataFrame({'hit_id': self.hits['hit_id'][event_slice]}, copy=False).set_index('hit_id')\n",
    "        x = torch.zeros((length, 3), dtype=self.float_dtype)\n",
    "        x[:,0] = torch.from_numpy(self.hits['x'][event_slice]) * self.scaling_factor\n",
    "        x[:,1] = torch.from_numpy(self.hits['y'][event_slice]) * self.scaling_factor\n",
    "        x[:,2] = torch.from_numpy(self.hits['z'][event_slice]) * self.scaling_factor\n",
    "        return x, hit_id\n",
    "\n",
    "    def _get_particle_ids(self, idx: int, detected_hits: pd.DataFrame):\n",
    "        # Note: not all hits in \"hits\" are also in \"truth\", and reciprocally\n",
    "        # Note: the weight is ignored for now\n",
    "        offset = self.truth['event_offset'][idx]\n",
    "        length = self.truth['event_length'][idx]\n",
    "        event_slice = slice(offset, offset+length)\n",
    "        truth = pd.DataFrame({\n",
    "            'hit_id': self.truth['hit_id'][event_slice],\n",
    "            'particle_id': self.truth['particle_id'][event_slice],\n",
    "        }, copy=False).set_index('hit_id')\n",
    "        # Letâ€™s find the true particle_id corresponding to each detected hit_id\n",
    "        joined = detected_hits.join(truth, on='hit_id', how='inner')\n",
    "        assert joined['particle_id'].dtype == truth['particle_id'].dtype\n",
    "        matched_particle_id = torch.from_numpy(joined['particle_id'].values)\n",
    "        return matched_particle_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class WedgePatchify3d:\n",
    "    \"\"\"\n",
    "    A class to transform hitwise data into wedges of annuli.\n",
    "\n",
    "    Give an event of shape [num_hits, 3] (x, y, z), return two mask tensors (context, target) of shape [num_hits]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, phi_range: float, eta_range: float, radius_midpoint: float, random_context: bool = True):\n",
    "        self.phi_range = phi_range\n",
    "        self.eta_range = eta_range\n",
    "        self.radius_midpoint = radius_midpoint\n",
    "        self.random_context = random_context\n",
    "\n",
    "    def __call__(self, sample: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Apply the WedgePatchify transform to the input sample.\n",
    "\n",
    "        Args:\n",
    "            sample (Dict): A dictionary containing hitwise data. Must include an 'x' key\n",
    "                           with a tensor of shape (num_hits, 3).\n",
    "\n",
    "        Returns:\n",
    "            Dict: The transformed sample with context, target, and mask tensors.\n",
    "        \"\"\"\n",
    "        x, y, z = self._extract_coordinates(sample)\n",
    "        radius, phi, eta = self._calculate_radius_phi_and_eta(x, y, z)\n",
    "        selected_phi = self._select_random_phi(phi)\n",
    "        phi_mask = self._create_phi_mask(phi, selected_phi)\n",
    "        selected_eta = self._select_random_eta(eta)\n",
    "        eta_mask = self._create_eta_mask(eta, selected_eta)\n",
    "        inner_mask, outer_mask = self._create_radius_masks(radius)\n",
    "        context_mask, target_mask = self._assign_masks(inner_mask, outer_mask, phi_mask, eta_mask)\n",
    "\n",
    "        sample[\"context_mask\"] = context_mask\n",
    "        sample[\"target_mask\"] = target_mask\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def _extract_coordinates(self, sample: Dict) -> (torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        x = sample[\"x\"][:, 0]\n",
    "        y = sample[\"x\"][:, 1]\n",
    "        z = sample[\"x\"][:, 2]\n",
    "        return x, y, z\n",
    "\n",
    "    def _calculate_radius_phi_and_eta(self, x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> (torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        radius = torch.sqrt(x**2 + y**2)\n",
    "        norm = torch.sqrt(x**2 + y**2 + z**2)\n",
    "        phi = torch.atan2(y, x)  # Returns values between -pi and pi\n",
    "        eta = torch.atanh(z / norm)\n",
    "        return radius, phi, eta\n",
    "\n",
    "    def _select_random_phi(self, phi: torch.Tensor) -> torch.Tensor:\n",
    "        hit_idx = torch.randint(0, phi.shape[0], (1,))\n",
    "        return phi[hit_idx]\n",
    "\n",
    "    def _select_random_eta(self, eta: torch.Tensor) -> torch.Tensor:\n",
    "        hit_idx = torch.randint(0, eta.shape[0], (1,))\n",
    "        return eta[hit_idx]\n",
    "\n",
    "    def _create_phi_mask(self, phi: torch.Tensor, selected_phi: torch.Tensor) -> torch.Tensor:\n",
    "        phi_min = selected_phi - self.phi_range / 2\n",
    "        phi_max = selected_phi + self.phi_range / 2\n",
    "        return torch.logical_or(\n",
    "            torch.logical_and(phi >= phi_min, phi <= phi_max),\n",
    "            torch.logical_and(phi + 2*torch.pi >= phi_min, phi + 2*torch.pi <= phi_max)\n",
    "        )\n",
    "\n",
    "    def _create_eta_mask(self, eta: torch.Tensor, selected_eta: torch.Tensor) -> torch.Tensor:\n",
    "        eta_min = selected_eta - self.eta_range / 2\n",
    "        eta_max = selected_eta + self.eta_range / 2\n",
    "        return torch.logical_and(eta >= eta_min, eta <= eta_max)\n",
    "\n",
    "    def _create_radius_masks(self, radius: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        inner_mask = radius <= self.radius_midpoint\n",
    "        outer_mask = radius > self.radius_midpoint\n",
    "        return inner_mask, outer_mask\n",
    "\n",
    "    def _assign_masks(self, inner_mask: torch.Tensor, outer_mask: torch.Tensor, phi_mask: torch.Tensor, eta_mask: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        if self.random_context and torch.rand(1).item() > 0.5:\n",
    "            context_mask = inner_mask & phi_mask & eta_mask\n",
    "            target_mask = outer_mask & phi_mask & eta_mask\n",
    "        else:\n",
    "            context_mask = outer_mask & phi_mask & eta_mask\n",
    "            target_mask = inner_mask & phi_mask & eta_mask\n",
    "        return context_mask, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackMLDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file: Union[str, Path],\n",
    "        scaling_factor: float = 1.0,\n",
    "        phi_range: float = 0.5,\n",
    "        eta_range: float = 0.5,\n",
    "        radius_midpoint: float = 500.0,\n",
    "        random_context: bool = True,\n",
    "        float_dtype=torch.float32,\n",
    "    ):\n",
    "        super(TrackMLDataset).__init__()\n",
    "        self.file = h5py.File(file, 'r')\n",
    "        self.number_of_events = self.file.attrs['number_of_events']\n",
    "        self.hits = self.file['hits']\n",
    "        self.truth = self.file['truth']\n",
    "        self.float_dtype = float_dtype\n",
    "        self.scaling_factor = torch.tensor(scaling_factor, dtype=float_dtype)\n",
    "        \n",
    "        # Store wedge parameters directly in the dataset\n",
    "        self.phi_range = phi_range\n",
    "        self.eta_range = eta_range\n",
    "        self.radius_midpoint = radius_midpoint\n",
    "        self.random_context = random_context\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_events\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x, hit_id = self._get_hits(idx)\n",
    "        # pids = self._get_particle_ids(idx, hit_id) #TODO Do we need it?\n",
    "        \n",
    "        # Calculate splits directly in the dataset\n",
    "        x_context, x_target, context_mask, target_mask = self._split_data(x)\n",
    "        \n",
    "        return {\n",
    "            'x_context': x_context,\n",
    "            'x_target': x_target,\n",
    "            'x_context_mask': torch.ones(x_context.shape[0], dtype=bool),\n",
    "            'x_target_mask': torch.ones(x_target.shape[0], dtype=bool),\n",
    "            # 'pids': pids,\n",
    "            'event': None,\n",
    "        }\n",
    "\n",
    "    def _get_hits(self, idx: int):\n",
    "        offset = self.hits['event_offset'][idx]\n",
    "        length = self.hits['event_length'][idx]\n",
    "        event_slice = slice(offset, offset+length)\n",
    "        hit_id = pd.DataFrame({'hit_id': self.hits['hit_id'][event_slice]}, copy=False).set_index('hit_id')\n",
    "        x = torch.zeros((length, 3), dtype=self.float_dtype)\n",
    "        x[:,0] = torch.from_numpy(self.hits['x'][event_slice]) * self.scaling_factor\n",
    "        x[:,1] = torch.from_numpy(self.hits['y'][event_slice]) * self.scaling_factor\n",
    "        x[:,2] = torch.from_numpy(self.hits['z'][event_slice]) * self.scaling_factor\n",
    "        return x, hit_id\n",
    "\n",
    "    def _get_particle_ids(self, idx: int, detected_hits: pd.DataFrame):\n",
    "        offset = self.truth['event_offset'][idx]\n",
    "        length = self.truth['event_length'][idx]\n",
    "        event_slice = slice(offset, offset+length)\n",
    "        truth = pd.DataFrame({\n",
    "            'hit_id': self.truth['hit_id'][event_slice],\n",
    "            'particle_id': self.truth['particle_id'][event_slice],\n",
    "        }, copy=False).set_index('hit_id')\n",
    "        joined = detected_hits.join(truth, on='hit_id', how='inner')\n",
    "        assert joined['particle_id'].dtype == truth['particle_id'].dtype\n",
    "        matched_particle_id = torch.from_numpy(joined['particle_id'].values)\n",
    "        return matched_particle_id\n",
    "\n",
    "    def _split_data(self, x: torch.Tensor):\n",
    "        \"\"\"Split the data into context and target based on geometric criteria\"\"\"\n",
    "        # Extract coordinates\n",
    "        x_coord, y_coord, z_coord = x[:, 0], x[:, 1], x[:, 2]\n",
    "        \n",
    "        # Calculate geometric quantities\n",
    "        radius = torch.sqrt(x_coord**2 + y_coord**2)\n",
    "        norm = torch.sqrt(x_coord**2 + y_coord**2 + z_coord**2)\n",
    "        phi = torch.atan2(y_coord, x_coord)\n",
    "        eta = torch.atanh(z_coord / norm)\n",
    "        \n",
    "        # Select random reference points\n",
    "        selected_phi = phi[torch.randint(0, phi.shape[0], (1,))]\n",
    "        selected_eta = eta[torch.randint(0, eta.shape[0], (1,))]\n",
    "        \n",
    "        # Create masks\n",
    "        phi_mask = self._create_phi_mask(phi, selected_phi)\n",
    "        eta_mask = self._create_eta_mask(eta, selected_eta)\n",
    "        inner_mask = radius <= self.radius_midpoint\n",
    "        outer_mask = radius > self.radius_midpoint\n",
    "        \n",
    "        # Assign context and target based on random selection\n",
    "        if self.random_context and torch.rand(1).item() > 0.5:\n",
    "            context_mask = inner_mask & phi_mask & eta_mask\n",
    "            target_mask = outer_mask & phi_mask & eta_mask\n",
    "        else:\n",
    "            context_mask = outer_mask & phi_mask & eta_mask\n",
    "            target_mask = inner_mask & phi_mask & eta_mask\n",
    "        \n",
    "        # Split the data based on masks\n",
    "        x_context = x[context_mask]\n",
    "        x_target = x[target_mask]\n",
    "        \n",
    "        return x_context, x_target, context_mask, target_mask\n",
    "\n",
    "    def _create_phi_mask(self, phi: torch.Tensor, selected_phi: torch.Tensor) -> torch.Tensor:\n",
    "        phi_min = selected_phi - self.phi_range / 2\n",
    "        phi_max = selected_phi + self.phi_range / 2\n",
    "        return torch.logical_or(\n",
    "            torch.logical_and(phi >= phi_min, phi <= phi_max),\n",
    "            torch.logical_and(phi + 2*torch.pi >= phi_min, phi + 2*torch.pi <= phi_max)\n",
    "        )\n",
    "\n",
    "    def _create_eta_mask(self, eta: torch.Tensor, selected_eta: torch.Tensor) -> torch.Tensor:\n",
    "        eta_min = selected_eta - self.eta_range / 2\n",
    "        eta_max = selected_eta + self.eta_range / 2\n",
    "        return torch.logical_and(eta >= eta_min, eta <= eta_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = TrackMLDataset('/home/ucloud/Particle-JEPA/data/TrackML/training-small.hdf5', transform=patchify, scaling_factor=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.random.manual_seed(42)\n",
    "# patchify = WedgePatchify3d(phi_range=torch.pi/2, eta_range=0.5, radius_midpoint=0.5) # Very approximate midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrackMLDataset(\n",
    "    file='/home/ucloud/Particle-JEPA/data/TrackML/training-small.hdf5',\n",
    "    scaling_factor=1e-3,\n",
    "    phi_range=torch.pi/2,\n",
    "    eta_range=0.5,\n",
    "    radius_midpoint=0.5,\n",
    "    random_context=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_context', 'x_target', 'x_context_mask', 'x_target_mask', 'event'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = dataset[0]\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([949, 3]),\n",
       " torch.Size([1035, 3]),\n",
       " torch.Size([949]),\n",
       " torch.Size([1035]),\n",
       " None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['x_context'].shape, batch['x_target'].shape, batch['x_context_mask'].shape, batch['x_target_mask'].shape, batch['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Dict]) -> Dict:\n",
    "    collated = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key == 'event':\n",
    "            collated[key] = [item[key] for item in batch]\n",
    "        else:\n",
    "            # Get first item to check dimensionality\n",
    "            first_item = batch[0][key]\n",
    "            if isinstance(first_item, torch.Tensor) and first_item.dim() == 0:\n",
    "                # For scalar tensors, stack them\n",
    "                collated[key] = torch.stack([item[key] for item in batch])\n",
    "            else:\n",
    "                # For tensors with dimensions, pad them\n",
    "                collated[key] = torch.nn.utils.rnn.pad_sequence([item[key] for item in batch], batch_first=True)\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_manual(batch: List[Dict]) -> Dict:\n",
    "    \"\"\"Current manual padding approach\"\"\"\n",
    "    batch_size = len(batch)\n",
    "    collated = {}\n",
    "    \n",
    "    # Handle x_context and x_target\n",
    "    for key in ['x_context', 'x_target']:\n",
    "        tensors = [item[key] for item in batch]\n",
    "        max_length = max(tensor.size(0) for tensor in tensors)\n",
    "        padded = torch.zeros(batch_size, max_length, 3, dtype=tensors[0].dtype)\n",
    "        for i, tensor in enumerate(tensors):\n",
    "            padded[i, :tensor.size(0)] = tensor\n",
    "        collated[key] = padded\n",
    "\n",
    "    # Handle masks\n",
    "    for key in ['x_context_mask', 'x_target_mask']:\n",
    "        tensors = [item[key] for item in batch]\n",
    "        max_length = max(tensor.size(0) for tensor in tensors)\n",
    "        padded = torch.zeros(batch_size, max_length, dtype=torch.bool)\n",
    "        for i, tensor in enumerate(tensors):\n",
    "            padded[i, :tensor.size(0)] = tensor\n",
    "        collated[key] = padded\n",
    "\n",
    "    collated['num_context'] = torch.tensor([item['x_context'].size(0) for item in batch])\n",
    "    collated['num_target'] = torch.tensor([item['x_target'].size(0) for item in batch])\n",
    "    \n",
    "    return collated\n",
    "\n",
    "def collate_fn_pad_sequence(batch: List[Dict]) -> Dict:\n",
    "    \"\"\"Alternative approach using pad_sequence\"\"\"\n",
    "    collated = {}\n",
    "    \n",
    "    # Handle x_context and x_target\n",
    "    for key in ['x_context', 'x_target']:\n",
    "        # Need to handle 3D tensors carefully\n",
    "        tensors = [item[key] for item in batch]\n",
    "        # pad_sequence expects sequence first, so we need to handle the 3D nature carefully\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "        collated[key] = padded\n",
    "\n",
    "    # Handle masks\n",
    "    for key in ['x_context_mask', 'x_target_mask']:\n",
    "        tensors = [item[key] for item in batch]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "        collated[key] = padded\n",
    "\n",
    "    collated['num_context'] = torch.tensor([item['x_context'].size(0) for item in batch])\n",
    "    collated['num_target'] = torch.tensor([item['x_target'].size(0) for item in batch])\n",
    "    \n",
    "    return collated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's add a timing test\n",
    "import time\n",
    "\n",
    "def compare_collate_methods(batch_size=32, num_points=100, num_trials=100):\n",
    "    # Create some dummy data\n",
    "    batch = []\n",
    "    for _ in range(batch_size):\n",
    "        # Random number of points between 1 and num_points\n",
    "        n_context = torch.randint(1, num_points, (1,)).item()\n",
    "        n_target = torch.randint(1, num_points, (1,)).item()\n",
    "        \n",
    "        batch.append({\n",
    "            'x_context': torch.randn(n_context, 3),\n",
    "            'x_target': torch.randn(n_target, 3),\n",
    "            'x_context_mask': torch.ones(n_context, dtype=torch.bool),\n",
    "            'x_target_mask': torch.ones(n_target, dtype=torch.bool),\n",
    "        })\n",
    "    \n",
    "    # Time manual padding\n",
    "    start = time.time()\n",
    "    for _ in range(num_trials):\n",
    "        _ = collate_fn_manual(batch)\n",
    "    manual_time = (time.time() - start) / num_trials\n",
    "    \n",
    "    # Time pad_sequence\n",
    "    start = time.time()\n",
    "    for _ in range(num_trials):\n",
    "        _ = collate_fn_pad_sequence(batch)\n",
    "    pad_sequence_time = (time.time() - start) / num_trials\n",
    "    \n",
    "    print(f\"Manual padding: {manual_time*1000:.2f}ms per batch\")\n",
    "    print(f\"pad_sequence:   {pad_sequence_time*1000:.2f}ms per batch\")\n",
    "    print(f\"Ratio (pad_sequence/manual): {pad_sequence_time/manual_time:.2f}x\")\n",
    "    \n",
    "    # Verify outputs are the same\n",
    "    out1 = collate_fn_manual(batch)\n",
    "    out2 = collate_fn_pad_sequence(batch)\n",
    "    \n",
    "    all_equal = all(torch.allclose(out1[k], out2[k]) for k in out1.keys())\n",
    "    print(f\"\\nOutputs are {'equal' if all_equal else 'different'}\")\n",
    "    \n",
    "    return manual_time, pad_sequence_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual padding: 0.74ms per batch\n",
      "pad_sequence:   0.52ms per batch\n",
      "Ratio (pad_sequence/manual): 0.71x\n",
      "\n",
      "Outputs are equal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0007410573959350585, 0.0005229401588439941)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_collate_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = collate_fn_pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_context': tensor([[[ 0.0076,  0.0311,  0.0587],\n",
       "          [ 0.0038,  0.0313,  0.0680],\n",
       "          [ 0.0036,  0.0313,  0.0480],\n",
       "          ...,\n",
       "          [-0.1734,  0.4673,  1.0042],\n",
       "          [-0.3437,  0.3612,  1.0078],\n",
       "          [-0.4036,  0.2946,  1.0438]],\n",
       " \n",
       "         [[-0.1410, -0.0819, -0.8180],\n",
       "          [-0.1510, -0.0818, -0.8180],\n",
       "          [-0.1409, -0.0878, -0.8180],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]),\n",
       " 'x_target': tensor([[[-0.1893,  0.4632,  0.6666],\n",
       "          [-0.3867,  0.3188,  0.6678],\n",
       "          [ 0.1439,  0.4800,  0.7072],\n",
       "          ...,\n",
       "          [-0.8520,  0.4263,  2.1555],\n",
       "          [-0.8724,  0.4073,  2.1555],\n",
       "          [-0.8580,  0.3647,  2.1555]],\n",
       " \n",
       "         [[-0.6131, -0.3374, -2.9485],\n",
       "          [-0.5623, -0.3127, -2.9485],\n",
       "          [-0.5434, -0.2653, -2.9485],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]),\n",
       " 'x_context_mask': tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ..., False, False, False]]),\n",
       " 'x_target_mask': tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ..., False, False, False]]),\n",
       " 'num_context': tensor([1567, 1250]),\n",
       " 'num_target': tensor([740, 445])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_batch_to_device(batch, device):\n",
    "    \"\"\"\n",
    "    Recursively moves all tensors in a batch to the specified device.\n",
    "    \n",
    "    Args:\n",
    "        batch: A dictionary, list, tuple, or tensor\n",
    "        device: The target device (e.g., 'cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        The batch with all tensors moved to the specified device\n",
    "    \"\"\"\n",
    "    if isinstance(batch, dict):\n",
    "        return {k: move_batch_to_device(v, device) for k, v in batch.items()}\n",
    "    elif isinstance(batch, (list, tuple)):\n",
    "        return type(batch)(move_batch_to_device(v, device) for v in batch)\n",
    "    elif hasattr(batch, 'to'):\n",
    "        return batch.to(device)\n",
    "    return batch\n",
    "\n",
    "# Usage example:\n",
    "# batch = move_batch_to_device(batch, device='cuda')  # For GPU\n",
    "# batch = move_batch_to_device(batch, device='cpu')   # For CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/9_testing.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['d_input'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JEA(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JEA(\n",
       "  (encoder): Encoder(\n",
       "    (transformer): Transformer(\n",
       "      (input_encoder): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): SiLU()\n",
       "        (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "      )\n",
       "      (encoder_layers): ModuleList(\n",
       "        (0-2): 3 x AttentionBlock(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (norm_self_attn): SetNorm()\n",
       "          (norm_ff): SetNorm()\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (aggregator): Aggregator(\n",
       "      (encoder_layers): ModuleList(\n",
       "        (0-1): 2 x AttentionBlock(\n",
       "          (cross_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (norm_cross_attn): SetNorm()\n",
       "          (norm_ff): SetNorm()\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (embeddings_encoder): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): SiLU()\n",
       "        (3): Linear(in_features=128, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ema_encoder): Encoder(\n",
       "    (transformer): Transformer(\n",
       "      (input_encoder): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): SiLU()\n",
       "        (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "      )\n",
       "      (encoder_layers): ModuleList(\n",
       "        (0-2): 3 x AttentionBlock(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (norm_self_attn): SetNorm()\n",
       "          (norm_ff): SetNorm()\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (aggregator): Aggregator(\n",
       "      (encoder_layers): ModuleList(\n",
       "        (0-1): 2 x AttentionBlock(\n",
       "          (cross_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (norm_cross_attn): SetNorm()\n",
       "          (norm_ff): SetNorm()\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (embeddings_encoder): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): SiLU()\n",
       "        (3): Linear(in_features=128, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# batch = move_batch_to_device(batch, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting first training step...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Particle-JEPA/jepa/modules/models/jea.py:129\u001b[0m, in \u001b[0;36mJEA.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting first training step...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Get inputs\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m x, mask, context_mask, target_mask, pids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_batch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug_batch_shapes(x, mask, context_mask, target_mask, pids)\n",
      "File \u001b[0;32m~/Particle-JEPA/jepa/modules/models/jea.py:241\u001b[0m, in \u001b[0;36mJEA._extract_batch_data\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_batch_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    Extracts necessary data from the batch.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m        Tuple containing x, mask, context_mask, target_mask and pids\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     x, mask, context_mask, target_mask, pids \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 241\u001b[0m         \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    242\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    243\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    244\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    245\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# Check for any batch entries that have all false context or target masks\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     valid_rows \u001b[38;5;241m=\u001b[39m context_mask\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m target_mask\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'x'"
     ]
    }
   ],
   "source": [
    "model.training_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
